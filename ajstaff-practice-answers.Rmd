---
title: "Midterm III Study Guide and Review"
author: "Deepak Bastola"
date: "`r format(Sys.Date(), ' %B %d %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

## Exam III Study Guide

Format: In class on Wed. 03/09, with open notes, open course resources and open R. No Google is allowed. You can use a calculator for mathematical calculations, if needed. You can use R for calculations too!

### Topics

- The exam covers baSIC web scraping, basic Shiny structure, classification algorithms including k-nearest neighbors, logistic regression, random forest and unsupervised learning algorithm, especially k-means. (through Mon 03/07). 

- You will be tested on your understanding of the R code and machine learning algorithms we have discussed. You will **not** be tested on complicated R-codes. Additional ways I could assess your understanding of R include (but are not limited to):

  - Filling in missing arguments/lines of code.
  - Identifying the error in written code.
  - Putting lines of code in order to complete a specified task.
  - Describing the output resulting from a code chunk.

*Disclaimer:* This list may not be exhaustive, but I hope that you find it useful in preparing for the exam.

1. Shiny. Understand the flow between input objects and rendered objects. Do not worry about layout options but make sure you understand the use of basic inputs like select, slider or radio buttons.

2. Scraping data from the web. Be able to read or write (basic) code to scrape data using `rvest`/`polite`. I wonâ€™t be asking questions about obscure HTML tags or CSS nodes, so focus on the basics/fundamentals like how you go from an HTML page to a data frame.

3. Classification (supervised learning). Understand the goals of classification problems and how to evaluate a classifier using the metrics discussed in class (accuracy, sensitivity, specificity, precision). Understand what each of the following classification methods is doing, how they compare, and how to implement them in R (reading the output is key here): knn, logistic regression, and random forests. Be able to explain how each of these algorithm works.

4. Cross validation. Be able to explain how the CV algorithm works. Why do we create a training and testing data set? How does k-fold cross validation work? How can you use k-fold cross validation to tune your classifier?

5. Unsupervised learning (k-means clustering). Understand the goal of unsupervised learning and how the k-means algorithm works.


```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse=TRUE, warning = FALSE, 
                      message=FALSE, include=TRUE, 
                      fig.height = 4, fig.width = 5)
# libraries
library(tidyverse)
library(tidymodels)
library(probably)
library(ISLR) # For the Smarket data set
library(rvest)
library(polite)
select <- dplyr::select
set.seed(1234)
```

\newpage

# Sample Midterm


## Q1. Briefly describe what the following set of codes do. Why do we need to split the data into training and test set?

```{r}
data_Smarket <- Smarket
split <- initial_split(data_Smarket, strata = Direction, prop = 4/5)
Smarket_train <- training(split)
Smarket_test <- testing(split)
```

*Answer:*

## Q2. Evaluate the following trained model to produce a data-frame of the actual and predicted `Direction` in the test dataset. Call this data-frame `Smarket_results`.

```{r}
Smarket_recipe <- recipe(Direction ~ Lag1 + Lag2 + Lag3 + Year + Volume, data = Smarket_train) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>%
  prep()
Smarket_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)
Smarket_workflow <- workflow() %>% 
  add_recipe(Smarket_recipe) %>%
  add_model(Smarket_knn_spec)
Smarket_fit <- fit(Smarket_workflow, data = Smarket_train)
```


```{r}
# Evaluate
# Your code here
```


Q3. Construct a confusion matrix from the prediction results from Q2. Calculate by hand the sensitivity, specificity, accuracy, and positive predictive value of the classifier.

*Answer:* Your answer here..



Q4. Now, run a 10-fold cross validation using appropriate functions in `tidymodels` package by adding `tune()` as a placeholder for the number of neighbors. Find an optimal number of nearest neighbor. What metrics did you base your conclusion on?

```{r}
# Your code here
```


Q5. Give your answers to the following set of problems.

a. Explain the difference between unsupervised learning and supervised learning. 

*Answer:*



b. Explain how you can use total within cluster sum of squares to find the "best" choice of K in a K-means clustering algorithm.

*Answer:*


c. Suppose your friend wants to design an app that allows the user to set a number (x) between 1 and 1000, and displays it's logarithm (base 10). His trial produced an error. Modify the following code to rectify the error.

```{r, eval=FALSE}
library(shiny)
ui <- fluidPage(
  sliderInput("x", label = "If x is", min = 1, max = 1000, value = 100),
  "then the logarithm of x is",
  textOutput("logarithm")
)
server <- function(input, output, session) {
  output$text <- renderText({ 
    log10(x)
  })
}
shinyApp(ui, server)
```


d. Briefly explain why do we preprocess data in k nearest neighbors algorithm.

*Answer:*



e. What does the following chunk of code do?

```{r}
table_covid_cases <- bow(url = "https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/state/minnesota") %>% 
  scrape() %>%
  html_elements(css = "table") %>%
  html_table()
```

*Answer:*
